# Лабораторная работа №1

## Датасет
Для решения задачи регрессии используется датасет для предсказания расхода топлива в милях на галлон. Здесь [ссылка](https://www.kaggle.com/datasets/uciml/autompg-dataset) на датасет.

## Реализация метода: бэггинг

Реализован ансамблевый алгоритм бэггинга, добавляющий модель в ансамбль по следующей логике:

1. Выбирается необходимое число базовых моделей (`n_estimators`), устанавливаются пороги качества моделей на обучающей и тестовой выборке (`base_train_threshold` и `base_test_threshold`)

2. Пока не наберется `n_estimators` базовых моделей:
    * Выбирается подвыборка из тренировочной выборки размерностью 2/3 от оригинальной
    * Модель на данной подвыборке обучается и валидируется
    * Если пороги качества модели удовлетворяют установленным, то модель добавляется в список, иначе продолжить. 


## Сравнение

### Время работы
* Время, затраченное кастомным алгоритмом: 324.568 мс
* Время, затраченное Sklearn алгоритмом: 97.897 мс

Кастомная реализация работает дольше, чем реализация в Sklearn. При этом если понизить пороги, указанные при инициализации класса (`BaggingRegressor(self, n_estimators=10, base_train_threshold=0.8, base_test_threshold=0.75)`), то время работы сократится. 

### Качество работы
Качество работы алгоритма определяется при помощи кросс-валидации. Для этого была написана следующая функция:
```python
def cross_validate(model, X, y, n_folds=5):
    scores = []
    for n in range(n_folds):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        model.fit(X_train, y_train)
        scores += [r2_score(model.predict(X_test), y_test)]
    return scores
```

Использовалась валидация на 10 выборках, оценка модели выполняется через метрику R2-score.
* Среднее R2-score для 10 выборок у кастомного алгоритма: 0.938
* Среднее R2-score для 10 выборок у Sklearn алгоритма: 0.834

Кастомный алгоритм работает лучше, это также связано с достаточно крупными минимальными параметрами, по которым модель добавляется в ансамбль.
